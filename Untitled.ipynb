{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a36e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06e4fc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR='images/train'\n",
    "TEST_DIR='images/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8e81e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createdataframe(dir):\n",
    "    image_paths=[]\n",
    "    labels=[]\n",
    "    for label in os.listdir(dir):\n",
    "        for imagename in os.listdir(os.path.join(dir,label)):\n",
    "            image_paths.append(os.path.join(dir,label,imagename))\n",
    "            labels.append(label)\n",
    "        print(label,\"completed\")\n",
    "    return image_paths,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a625bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n"
     ]
    }
   ],
   "source": [
    "train=pd.DataFrame()\n",
    "train['image'],train['label']=createdataframe(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8ab8ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                image     label\n",
      "0            images/train\\angry\\0.jpg     angry\n",
      "1            images/train\\angry\\1.jpg     angry\n",
      "2           images/train\\angry\\10.jpg     angry\n",
      "3        images/train\\angry\\10002.jpg     angry\n",
      "4        images/train\\angry\\10016.jpg     angry\n",
      "...                               ...       ...\n",
      "28816  images/train\\surprise\\9969.jpg  surprise\n",
      "28817  images/train\\surprise\\9985.jpg  surprise\n",
      "28818  images/train\\surprise\\9990.jpg  surprise\n",
      "28819  images/train\\surprise\\9992.jpg  surprise\n",
      "28820  images/train\\surprise\\9996.jpg  surprise\n",
      "\n",
      "[28821 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36359ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n"
     ]
    }
   ],
   "source": [
    "test=pd.DataFrame()\n",
    "test['image'],test['label']=createdataframe(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c354adc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              image     label\n",
      "0       images/test\\angry\\10052.jpg     angry\n",
      "1       images/test\\angry\\10065.jpg     angry\n",
      "2       images/test\\angry\\10079.jpg     angry\n",
      "3       images/test\\angry\\10095.jpg     angry\n",
      "4       images/test\\angry\\10121.jpg     angry\n",
      "...                             ...       ...\n",
      "7061  images/test\\surprise\\9806.jpg  surprise\n",
      "7062  images/test\\surprise\\9830.jpg  surprise\n",
      "7063  images/test\\surprise\\9853.jpg  surprise\n",
      "7064  images/test\\surprise\\9878.jpg  surprise\n",
      "7065   images/test\\surprise\\993.jpg  surprise\n",
      "\n",
      "[7066 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b563f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd251c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(images):\n",
    "    features = []\n",
    "    for image in tqdm(images):\n",
    "        img = load_img(image,grayscale=True)  # updated to color_mode='grayscale'\n",
    "        img_array = img_to_array(img)\n",
    "        features.append(img_array)\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "561dc6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28821 [00:00<?, ?it/s]c:\\Users\\saduv\\emotion_detection\\myenv\\lib\\site-packages\\keras\\utils\\image_utils.py:410: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  'grayscale is deprecated. Please use color_mode = \"grayscale\"'\n",
      "100%|██████████| 28821/28821 [05:12<00:00, 92.30it/s] \n"
     ]
    }
   ],
   "source": [
    "train_features=extract_features(train['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23b31d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7066/7066 [01:17<00:00, 91.70it/s] \n"
     ]
    }
   ],
   "source": [
    "test_features=extract_features(test['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e453b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=train_features/255.0\n",
    "x_test=test_features/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06340335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dffadf10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le=LabelEncoder()\n",
    "le.fit(train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f319d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = le.transform(train['label'])\n",
    "y_test = le.transform(test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b76cb77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=to_categorical(y_train,num_classes=7)\n",
    "y_test=to_categorical(y_test,num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f45d1965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acf5262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "# convolutional layer\n",
    "model.add(Conv2D(128,kernel_size=(3,3),activation='relu',input_shape=(48,48,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256,kernel_size=(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512,kernel_size=(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512,kernel_size=(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "# fully connected layers\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "# output layer\n",
    "model.add(Dense(7,activation='softmax'))\n",
    "# 26:32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1939a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec5e645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4202c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoint filepath\n",
    "checkpoint_filepath = 'model_checkpoint.h5'\n",
    "\n",
    "# Define the ModelCheckpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,  # Save only the model weights\n",
    "    save_best_only=True,     # Save only the best model (based on validation loss)\n",
    "    monitor='val_loss',      # Monitor validation loss\n",
    "    mode='min',              # Mode for monitoring ('min' means we're looking for validation loss to decrease)\n",
    "    verbose=1                # Verbosity level\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca6badc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model_checkpoint.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c69ea35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.0933 - accuracy: 0.5867\n",
      "Epoch 1: val_loss improved from inf to 1.08324, saving model to model_checkpoint.h5\n",
      "226/226 [==============================] - 457s 2s/step - loss: 1.0933 - accuracy: 0.5867 - val_loss: 1.0832 - val_accuracy: 0.6005\n",
      "Epoch 2/100\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.0801 - accuracy: 0.5924\n",
      "Epoch 2: val_loss improved from 1.08324 to 1.06562, saving model to model_checkpoint.h5\n",
      "226/226 [==============================] - 458s 2s/step - loss: 1.0801 - accuracy: 0.5924 - val_loss: 1.0656 - val_accuracy: 0.6010\n",
      "Epoch 3/100\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.0715 - accuracy: 0.5977\n",
      "Epoch 3: val_loss did not improve from 1.06562\n",
      "226/226 [==============================] - 457s 2s/step - loss: 1.0715 - accuracy: 0.5977 - val_loss: 1.0816 - val_accuracy: 0.6027\n",
      "Epoch 4/100\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.0658 - accuracy: 0.5987\n",
      "Epoch 4: val_loss did not improve from 1.06562\n",
      "226/226 [==============================] - 389s 2s/step - loss: 1.0658 - accuracy: 0.5987 - val_loss: 1.0730 - val_accuracy: 0.6057\n",
      "Epoch 5/100\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.0630 - accuracy: 0.5946\n",
      "Epoch 5: val_loss did not improve from 1.06562\n",
      "226/226 [==============================] - 366s 2s/step - loss: 1.0630 - accuracy: 0.5946 - val_loss: 1.0667 - val_accuracy: 0.6100\n",
      "Epoch 6/100\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.0593 - accuracy: 0.6006\n",
      "Epoch 6: val_loss improved from 1.06562 to 1.04904, saving model to model_checkpoint.h5\n",
      "226/226 [==============================] - 364s 2s/step - loss: 1.0593 - accuracy: 0.6006 - val_loss: 1.0490 - val_accuracy: 0.6085\n",
      "Epoch 7/100\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.0491 - accuracy: 0.6036\n",
      "Epoch 7: val_loss did not improve from 1.04904\n",
      "226/226 [==============================] - 363s 2s/step - loss: 1.0491 - accuracy: 0.6036 - val_loss: 1.0495 - val_accuracy: 0.6084\n",
      "Epoch 8/100\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.0347 - accuracy: 0.6087\n",
      "Epoch 8: val_loss did not improve from 1.04904\n",
      "226/226 [==============================] - 364s 2s/step - loss: 1.0347 - accuracy: 0.6087 - val_loss: 1.0524 - val_accuracy: 0.6172\n",
      "Epoch 9/100\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.0337 - accuracy: 0.6047\n",
      "Epoch 9: val_loss did not improve from 1.04904\n",
      "226/226 [==============================] - 366s 2s/step - loss: 1.0337 - accuracy: 0.6047 - val_loss: 1.0571 - val_accuracy: 0.6043\n",
      "Epoch 10/100\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.0242 - accuracy: 0.6119\n",
      "Epoch 10: val_loss did not improve from 1.04904\n",
      "226/226 [==============================] - 361s 2s/step - loss: 1.0242 - accuracy: 0.6119 - val_loss: 1.0530 - val_accuracy: 0.6054\n",
      "Epoch 11/100\n",
      "146/226 [==================>...........] - ETA: 2:03 - loss: 1.0108 - accuracy: 0.6219"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1888\\2580905571.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Pass the ModelCheckpoint callback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;32mc:\\Users\\saduv\\emotion_detection\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saduv\\emotion_detection\\myenv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                         ):\n\u001b[0;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1651\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saduv\\emotion_detection\\myenv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saduv\\emotion_detection\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saduv\\emotion_detection\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saduv\\emotion_detection\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m    134\u001b[0m     return concrete_function._call_flat(\n\u001b[1;32m--> 135\u001b[1;33m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saduv\\emotion_detection\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saduv\\emotion_detection\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    381\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    384\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\Users\\saduv\\emotion_detection\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 53\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=x_train, \n",
    "    y=y_train, \n",
    "    batch_size=128, \n",
    "    epochs=100, \n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[checkpoint_callback]  # Pass the ModelCheckpoint callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47c1c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json=model.to_json()\n",
    "with open(\"emotiondetector.json\",'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save(\"emotiondetector.h5\")\n",
    "# 30:12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "855ed1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bc764b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('facialemotionmodel.json', 'r') as json_file:\n",
    "    model_json = json_file.read()\n",
    "\n",
    "# Load the model architecture\n",
    "model = model_from_json(model_json)\n",
    "\n",
    "# Load the weights into the model\n",
    "model.load_weights('facialemotionmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba5a3588",
   "metadata": {},
   "outputs": [],
   "source": [
    "label=[\"angry\",\"disgust\",\"fear\",\"happy\",\"neutral\",\"sad\",\"suprise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74a1939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0e5dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ef(image):\n",
    "#     img=load_img(image,color_mode = \"grayscale\")\n",
    "#     feature=np.array(img)\n",
    "#     feature=feature.reshape(1,48,48,1)\n",
    "#     return feature/255.0\n",
    "def preprocess_image(image_path, target_size):\n",
    "    # Load the image in grayscale\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    # Resize the image to the target size\n",
    "    img = cv2.resize(img, target_size)\n",
    "    # Expand dimensions to create a batch dimension\n",
    "    img = np.expand_dims(img, axis=-1)\n",
    "    # Normalize the image\n",
    "    img = img.astype(\"float\") / 255.0\n",
    "    # Add batch dimension\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2397df86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original is angry\n",
      "Preprocessed image shape: (1, 48, 48, 1)\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "Model prediction is: angry\n"
     ]
    }
   ],
   "source": [
    "# image='images/train/disgust/416.jpg'\n",
    "# print(\"original is disgust\")\n",
    "# img=ef(image)\n",
    "# pred=model.predict(image)\n",
    "# print(pred)\n",
    "# pred_label=label[pred.argmax()]\n",
    "# print(\"model prediction is \",pred_label)\n",
    "input_shape = model.input_shape\n",
    "target_size = (input_shape[1], input_shape[2])\n",
    "\n",
    "# Example usage\n",
    "image_path = 'images/train/angry/0.jpg'\n",
    "print(\"original is angry\")\n",
    "\n",
    "# Preprocess the image\n",
    "img = preprocess_image(image_path, target_size)\n",
    "\n",
    "# Verify the shape of the preprocessed image\n",
    "print(\"Preprocessed image shape:\", img.shape)\n",
    "\n",
    "# Predict using the model\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"Model prediction is:\", pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13dab417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d60eaeb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Convert the color space from BGR to RGB (OpenCV uses BGR by default)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the image using plt.imshow()\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
